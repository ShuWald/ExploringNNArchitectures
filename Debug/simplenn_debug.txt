[
  {
    "layer": 0,
    "message": "Initializing layer 0: 2 -> 3 neurons, activation: relu"
  },
  {
    "layer": 1,
    "message": "Initializing layer 1: 3 -> 1 neurons, activation: sigmoid"
  },
  {
    "input": [
      [
        0.5,
        -0.2
      ],
      [
        1.0,
        0.3
      ]
    ],
    "message": "Input to network"
  },
  {
    "layer": 0,
    "weights": [
      [
        0.004967141530112327,
        -0.0013826430117118466,
        0.006476885381006925
      ],
      [
        0.015230298564080254,
        -0.0023415337472333596,
        -0.0023413695694918055
      ]
    ],
    "biases": [
      [
        0.0,
        0.0,
        0.0
      ]
    ],
    "z": [
      [
        -0.0005624889477598876,
        -0.00022301475640925135,
        0.0037067166044018234
      ],
      [
        0.009536231099336403,
        -0.0020851031358818544,
        0.005774474510159383
      ]
    ],
    "activation_output": [
      [
        0.0,
        0.0,
        0.0037067166044018234
      ],
      [
        0.009536231099336403,
        0.0,
        0.005774474510159383
      ]
    ],
    "message": "Layer 0 forward pass"
  },
  {
    "layer": 1,
    "weights": [
      [
        0.015792128155073915
      ],
      [
        0.007674347291529088
      ],
      [
        -0.004694743859349522
      ]
    ],
    "biases": [
      [
        0.0
      ]
    ],
    "z": [
      [
        -1.740208501686437e-05
      ],
      [
        0.0001234877048895808
      ]
    ],
    "activation_output": [
      [
        0.49999564947874586
      ],
      [
        0.5000308719261832
      ]
    ],
    "message": "Layer 1 forward pass"
  },
  {
    "final_output": [
      [
        0.49999564947874586
      ],
      [
        0.5000308719261832
      ]
    ],
    "message": "Final output"
  }
]